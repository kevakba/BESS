{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1011887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook do inference on a trained model\n",
    "\n",
    "# NVIDIA-SMI 535.183.01 \n",
    "# CUDA Version: 10.1\n",
    "# CUDNN Version: \n",
    "    #define CUDNN_MAJOR 7\n",
    "    #define CUDNN_MINOR 6\n",
    "    #define CUDNN_PATCHLEVEL 5\n",
    "# Tensorflow Version: 2.2.0\n",
    "# Python Version: 3.8.10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe59bc9",
   "metadata": {},
   "source": [
    "#### Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3835c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import os\n",
    "\n",
    "# Set pandas to display all rows and columns\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Suppress pandas performance warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=pd.errors.PerformanceWarning)\n",
    "from datetime import datetime, timedelta\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a237cb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check tf version\n",
    "print('TensorFlow version:', tf.__version__)\n",
    "\n",
    "#  Check if TensorFlow can access GPUs\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"TensorFlow is using GPU: {gpus}\")\n",
    "else:\n",
    "    print(\"No GPU found, using CPU instead.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4740a5",
   "metadata": {},
   "source": [
    "#### Load Inference Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2705babd",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# load inference data\n",
    "inference_data = pd.read_csv('Jobs/Inferencing/data/cleaned/merged_df_cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083f6bf9",
   "metadata": {},
   "source": [
    "#### Model Inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ee834c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Load the saved model\n",
    "def load_model(save_path):\n",
    "    model = tf.keras.models.load_model(save_path)\n",
    "    print(f\"Model loaded from {save_path}\")\n",
    "    return model\n",
    "\n",
    "def get_artifact(folder_path, extension):\n",
    "    # Get all files with the specified extension in the folder\n",
    "    files = [f for f in os.listdir(folder_path) if f.endswith(extension)]\n",
    "    \n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No files with extension '{extension}' found in {folder_path}\")\n",
    "    \n",
    "    # Get the full paths of the files\n",
    "    full_paths = [os.path.join(folder_path, f) for f in files]\n",
    "    \n",
    "    # Find the file with the latest modification time\n",
    "    latest_file = max(full_paths, key=os.path.getmtime)\n",
    "    # # latest_file = [full_paths]\n",
    "    return latest_file\n",
    "\n",
    "# save_path = f\"Jobs/Retraining/artifacts/lstm_model_{datetime.now().strftime('%Y%m%d')}.h5\"\n",
    "folder_path = \"Jobs/Retraining/artifacts/\" \n",
    "# Get the latest .h5 model file\n",
    "latest_model_path = get_artifact(folder_path, extension=\".h5\")\n",
    "\n",
    "# load trained model\n",
    "model = load_model(latest_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188b6d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the scaler\n",
    "def load_scaler(save_path):\n",
    "    scaler = joblib.load(save_path)\n",
    "    print(f\"Scaler loaded from {save_path}\")\n",
    "    return scaler\n",
    "\n",
    "# save_path = f\"Jobs/Retraining/artifacts/scaler_{datetime.now().strftime('%Y%m%d')}.pkl\"\n",
    "folder_path = \"Jobs/Retraining/artifacts/\"\n",
    "# Get the latest .pkl scaler file\n",
    "latest_scaler_path = get_artifact(folder_path, extension=\".pkl\")\n",
    "\n",
    "scaler = load_scaler(latest_scaler_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f47c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data transformation\n",
    "X_inference = inference_data.drop(columns=['datetime_'])\n",
    "X_inference = scaler.transform(X_inference)\n",
    "X_inference = X_inference.reshape((X_inference.shape[0], 1, int(X_inference.shape[1])))\n",
    "\n",
    "# make predictions\n",
    "y_inference_pred = model.predict(X_inference)\n",
    "y_inference_pred = y_inference_pred.reshape(-1, 1)\n",
    "y_inference_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643b855e",
   "metadata": {
    "lines_to_next_cell": 3
   },
   "outputs": [],
   "source": [
    "# save the predictions\n",
    "pred_df_new = pd.DataFrame()\n",
    "pred_df_new['datetime_'] = inference_data['datetime_']\n",
    "pred_df_new['predicted_pool_price'] = y_inference_pred\n",
    "\n",
    "print(pred_df_new)\n",
    "\n",
    "# load the existing predictions\n",
    "try:\n",
    "    pred_df = pd.read_csv('Jobs/Inferencing/data/predictions/pred_df.csv')\n",
    "except:\n",
    "    pred_df = pd.DataFrame()\n",
    "\n",
    "print(pred_df_new)\n",
    "\n",
    "# concatenate the new predictions with the existing predictions\n",
    "pred_df = pd.concat([pred_df, pred_df_new], axis=0, ignore_index=True)\n",
    "# drop null values\n",
    "pred_df = pred_df.dropna(subset=['predicted_pool_price'])\n",
    "# drop duplicates\n",
    "pred_df = pred_df.drop_duplicates(subset=['datetime_'], keep='last')\n",
    "# sort the dataframe by datetime\n",
    "pred_df = pred_df.sort_values(by='datetime_', ascending=True)\n",
    "# reset the index\n",
    "pred_df = pred_df.reset_index(drop=True)\n",
    "# save the predictions\n",
    "pred_df.to_csv('Jobs/Inferencing/data/predictions/pred_df.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
